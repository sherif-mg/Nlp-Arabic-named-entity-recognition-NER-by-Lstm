{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQmIg2U-XP5n"
      },
      "outputs": [],
      "source": [
        "!mkdir Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download word2vec"
      ],
      "metadata": {
        "id": "9Oq0B0nU4FZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYjJjgG_zXwP",
        "outputId": "7c34e49f-26c8-4ade-9582-114fdaa5a028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 22:35:34--  https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_wiki.zip\n",
            "Resolving bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)... 108.61.0.122, 2001:19f0:0:22::100\n",
            "Connecting to bakrianoo.ewr1.vultrobjects.com (bakrianoo.ewr1.vultrobjects.com)|108.61.0.122|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1488871452 (1.4G) [application/zip]\n",
            "Saving to: ‘full_grams_sg_300_wiki.zip’\n",
            "\n",
            "full_grams_sg_300_w 100%[===================>]   1.39G   105MB/s    in 14s     \n",
            "\n",
            "2022-05-13 22:35:48 (98.4 MB/s) - ‘full_grams_sg_300_wiki.zip’ saved [1488871452/1488871452]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: bakrianoo.ewr1.vultrobjects.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: ar-AE,ar;q=0.9,en-US;q=0.8,en;q=0.7\" --header=\"Referer: https://github.com/bakrianoo/aravec\" \"https://bakrianoo.ewr1.vultrobjects.com/aravec/full_grams_sg_300_wiki.zip\" -c -O 'full_grams_sg_300_wiki.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Dataset"
      ],
      "metadata": {
        "id": "c4_R7LVm4M8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT0bHhhlWfCq",
        "outputId": "a17a7dea-843a-4fb4-9803-cfbba9adafff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 22:36:05--  http://www.cs.cmu.edu/~ark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7815886 (7.5M) [application/zip]\n",
            "Saving to: ‘AQMAR_Arabic_NER_corpus-1.0.zip’\n",
            "\n",
            "AQMAR_Arabic_NER_co 100%[===================>]   7.45M  3.09MB/s    in 2.4s    \n",
            "\n",
            "2022-05-13 22:36:08 (3.09 MB/s) - ‘AQMAR_Arabic_NER_corpus-1.0.zip’ saved [7815886/7815886]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --header=\"Host: www.cs.cmu.edu\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.54 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: ar-AE,ar;q=0.9,en-US;q=0.8,en;q=0.7\" --header=\"Referer: http://www.cs.cmu.edu/~ark/ArabicNER/\" --header=\"Cookie: BIGipServer~SCS~cs-userdir-pool-80=516555392.20480.0000; SHIBLOCATION=tilde; BALANCEID=balancer.web39.srv.cs.cmu.edu\" --header=\"Connection: keep-alive\" \"http://www.cs.cmu.edu/~ark/ArabicNER/AQMAR_Arabic_NER_corpus-1.0.zip\" -c -O 'AQMAR_Arabic_NER_corpus-1.0.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlK2Ep2zV-5J",
        "outputId": "1735320e-da8a-4aad-d64d-1c8a10d8a6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  AQMAR_Arabic_NER_corpus-1.0.zip\n",
            "  inflating: Data/Atom.txt           \n",
            "  inflating: Data/Christiano_Ronaldo.txt  \n",
            "  inflating: Data/Computer.txt       \n",
            "  inflating: Data/Computer_Software.txt  \n",
            "  inflating: Data/Crusades.txt       \n",
            "  inflating: Data/Damascus.txt       \n",
            "  inflating: Data/Enrico_Fermi.txt   \n",
            "  inflating: Data/Football.txt       \n",
            "  inflating: Data/Ibn_Tolun_Mosque.txt  \n",
            "  inflating: Data/Imam_Hussein_Shrine.txt  \n",
            "  inflating: Data/Internet.txt       \n",
            "  inflating: Data/Islamic_Golden_Age.txt  \n",
            "  inflating: Data/Islamic_History.txt  \n",
            "  inflating: Data/LICENSE            \n",
            "  inflating: Data/Light.txt          \n",
            "  inflating: Data/Linux.txt          \n",
            "  inflating: Data/Nuclear_Power.txt  \n",
            "  inflating: Data/Periodic_Table.txt  \n",
            "  inflating: Data/Physics.txt        \n",
            "  inflating: Data/Portugal_football_team.txt  \n",
            "  inflating: Data/README             \n",
            "  inflating: Data/Raul_Gonzales.txt  \n",
            "  inflating: Data/Razi.txt           \n",
            "  inflating: Data/Real_Madrid.txt    \n",
            "  inflating: Data/Richard_Stallman.txt  \n",
            "  inflating: Data/Soccer_Worldcup.txt  \n",
            "  inflating: Data/Solaris.txt        \n",
            "  inflating: Data/Summer_Olympics2004.txt  \n",
            "  inflating: Data/Ummaya_Mosque.txt  \n",
            "  inflating: Data/VERSION            \n",
            "  inflating: Data/X_window_system.txt  \n",
            "   creating: Data/featureFiles/\n",
            "   creating: Data/featureFiles/.svn/\n",
            "  inflating: Data/featureFiles/.svn/entries  \n",
            "   creating: Data/featureFiles/.svn/prop-base/\n",
            "   creating: Data/featureFiles/.svn/props/\n",
            "   creating: Data/featureFiles/.svn/text-base/\n",
            "  inflating: Data/featureFiles/.svn/text-base/README.svn-base  \n",
            "   creating: Data/featureFiles/.svn/tmp/\n",
            "   creating: Data/featureFiles/.svn/tmp/prop-base/\n",
            "   creating: Data/featureFiles/.svn/tmp/props/\n",
            "   creating: Data/featureFiles/.svn/tmp/text-base/\n",
            "   creating: Data/featureFiles/dev/\n",
            "   creating: Data/featureFiles/dev/.svn/\n",
            "  inflating: Data/featureFiles/dev/.svn/entries  \n",
            "   creating: Data/featureFiles/dev/.svn/prop-base/\n",
            "   creating: Data/featureFiles/dev/.svn/props/\n",
            "   creating: Data/featureFiles/dev/.svn/text-base/\n",
            "  inflating: Data/featureFiles/dev/.svn/text-base/all.dev.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/dev/.svn/text-base/history.dev.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/dev/.svn/text-base/science.dev.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/dev/.svn/text-base/sport.dev.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/dev/.svn/text-base/tech.dev.features.txt.svn-base  \n",
            "   creating: Data/featureFiles/dev/.svn/tmp/\n",
            "   creating: Data/featureFiles/dev/.svn/tmp/prop-base/\n",
            "   creating: Data/featureFiles/dev/.svn/tmp/props/\n",
            "   creating: Data/featureFiles/dev/.svn/tmp/text-base/\n",
            "  inflating: Data/featureFiles/dev/all.dev.features.txt  \n",
            "  inflating: Data/featureFiles/dev/history.dev.features.txt  \n",
            "  inflating: Data/featureFiles/dev/science.dev.features.txt  \n",
            "  inflating: Data/featureFiles/dev/sport.dev.features.txt  \n",
            "  inflating: Data/featureFiles/dev/tech.dev.features.txt  \n",
            "   creating: Data/featureFiles/lexicons/\n",
            "   creating: Data/featureFiles/lexicons/.svn/\n",
            "  inflating: Data/featureFiles/lexicons/.svn/entries  \n",
            "   creating: Data/featureFiles/lexicons/.svn/prop-base/\n",
            " extracting: Data/featureFiles/lexicons/.svn/prop-base/NEList.txt.svn-base  \n",
            " extracting: Data/featureFiles/lexicons/.svn/prop-base/NonNEList.txt.svn-base  \n",
            "   creating: Data/featureFiles/lexicons/.svn/props/\n",
            "   creating: Data/featureFiles/lexicons/.svn/text-base/\n",
            "  inflating: Data/featureFiles/lexicons/.svn/text-base/NEList.txt.svn-base  \n",
            "  inflating: Data/featureFiles/lexicons/.svn/text-base/NonNEList.txt.svn-base  \n",
            "   creating: Data/featureFiles/lexicons/.svn/tmp/\n",
            "   creating: Data/featureFiles/lexicons/.svn/tmp/prop-base/\n",
            "   creating: Data/featureFiles/lexicons/.svn/tmp/props/\n",
            "   creating: Data/featureFiles/lexicons/.svn/tmp/text-base/\n",
            "  inflating: Data/featureFiles/lexicons/NEList.txt  \n",
            "  inflating: Data/featureFiles/lexicons/NonNEList.txt  \n",
            "  inflating: Data/featureFiles/README  \n",
            "   creating: Data/featureFiles/test/\n",
            "   creating: Data/featureFiles/test/.svn/\n",
            "  inflating: Data/featureFiles/test/.svn/entries  \n",
            "   creating: Data/featureFiles/test/.svn/prop-base/\n",
            "   creating: Data/featureFiles/test/.svn/props/\n",
            "   creating: Data/featureFiles/test/.svn/text-base/\n",
            "  inflating: Data/featureFiles/test/.svn/text-base/all.test.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/test/.svn/text-base/history.test.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/test/.svn/text-base/science.test.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/test/.svn/text-base/sport.test.features.txt.svn-base  \n",
            "  inflating: Data/featureFiles/test/.svn/text-base/tech.test.features.txt.svn-base  \n",
            "   creating: Data/featureFiles/test/.svn/tmp/\n",
            "   creating: Data/featureFiles/test/.svn/tmp/prop-base/\n",
            "   creating: Data/featureFiles/test/.svn/tmp/props/\n",
            "   creating: Data/featureFiles/test/.svn/tmp/text-base/\n",
            "  inflating: Data/featureFiles/test/all.test.features.txt  \n",
            "  inflating: Data/featureFiles/test/history.test.features.txt  \n",
            "  inflating: Data/featureFiles/test/science.test.features.txt  \n",
            "  inflating: Data/featureFiles/test/sport.test.features.txt  \n",
            "  inflating: Data/featureFiles/test/tech.test.features.txt  \n",
            "Archive:  /content/full_grams_sg_300_wiki.zip\n",
            "  inflating: full_grams_sg_300_wiki.mdl  \n",
            "  inflating: full_grams_sg_300_wiki.mdl.trainables.syn1neg.npy  \n",
            "  inflating: full_grams_sg_300_wiki.mdl.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"AQMAR_Arabic_NER_corpus-1.0.zip\" -d \"Data\"\n",
        "!unzip \"/content/full_grams_sg_300_wiki.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq50L1XnCR2L",
        "outputId": "c33a7906-385f-4dc9-8a12-7727e0976fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting camel-tools\n",
            "  Downloading camel_tools-1.3.1-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 30.7 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.0.2)\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 27.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.11.0+cu113)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from camel-tools) (2.23.0)\n",
            "Collecting transformers>=3.0.2\n",
            "  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 62.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.3.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.4.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.8.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.16.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.6.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.2.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.5.3)\n",
            "Requirement already satisfied: pyrsistent in /usr/local/lib/python3.7/dist-packages (from camel-tools) (0.18.1)\n",
            "Collecting camel-kenlm\n",
            "  Downloading camel-kenlm-2021.12.27.tar.gz (418 kB)\n",
            "\u001b[K     |████████████████████████████████| 418 kB 60.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from camel-tools) (1.3.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from camel-tools) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->camel-tools) (4.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 61.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (3.6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.2->camel-tools) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->camel-tools) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->camel-tools) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel-tools) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->camel-tools) (1.1.0)\n",
            "Building wheels for collected packages: camel-kenlm, emoji\n",
            "  Building wheel for camel-kenlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for camel-kenlm: filename=camel_kenlm-2021.12.27-cp37-cp37m-linux_x86_64.whl size=2335620 sha256=783775a4fdddbf9c0a824ceb1d681545a44ae32b82912627b34af00991c22891\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/72/74/982f8c435f15b7feaf6dc8a03e212ff34e93f1f2d747059332\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=11ce2efe2ec2e5ff5e9c9b581f3868980c5279c4f2ca80e69b5f900ed9f72ce4\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built camel-kenlm emoji\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers, emoji, camel-kenlm, camel-tools\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed camel-kenlm-2021.12.27 camel-tools-1.3.1 emoji-1.7.0 huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.1\n"
          ]
        }
      ],
      "source": [
        "!pip install camel-tools\n",
        "#!camel_data --install --force all "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiKm8n4NC0-u"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import os\n",
        "import numpy as np\n",
        "import re \n",
        "from camel_tools.utils.normalize import normalize_alef_maksura_ar\n",
        "from camel_tools.utils.normalize import normalize_alef_ar\n",
        "from camel_tools.utils.normalize import normalize_teh_marbuta_ar\n",
        "from camel_tools.utils.dediac import dediac_ar\n",
        "from keras.layers.embeddings import Embedding\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset file by file and return list have all files \n",
        "output dimension(files,words in file)"
      ],
      "metadata": {
        "id": "hQSwPBEO4Uys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfBYDBR4rlHH"
      },
      "outputs": [],
      "source": [
        "def LoadDataset():\n",
        "  allTxt = []\n",
        "  folder_dir=\"/content/Data\"\n",
        "  for file in os.listdir(folder_dir):\n",
        "      if file.endswith(\"txt\"):\n",
        "          allTxt.append(open(os.path.join(folder_dir, file),'r').readlines())\n",
        "  return allTxt     "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "use this function beacuse Dataset have english words"
      ],
      "metadata": {
        "id": "q13BXnNQ40K8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuutjNeJSdQX"
      },
      "outputs": [],
      "source": [
        "def Translate(word,voc,WordStem):\n",
        "  if(word in voc):\n",
        "    return voc[word]\n",
        "  elif(WordStem(word) in voc):\n",
        "    return voc[WordStem(word)]\n",
        "  return WordStem(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0xpDltVWpWG"
      },
      "outputs": [],
      "source": [
        "def WordStemArabic(word):\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "  stemmer = SnowballStemmer(language=\"arabic\")\n",
        "  return  stemmer.stem(word)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cupxqn0RIjxC"
      },
      "outputs": [],
      "source": [
        "def WordStemEnglish(word):\n",
        "  from nltk.stem.snowball import SnowballStemmer\n",
        "  stemmer = SnowballStemmer(language=\"english\")\n",
        "  return stemmer.stem(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "load dictionary which translate english word into arabic\n",
        "this dictionary in dataset"
      ],
      "metadata": {
        "id": "BHJB8LFZ5Vew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6NzO9N0RRtK"
      },
      "outputs": [],
      "source": [
        "def Loadlexicons():\n",
        "  corpus = []\n",
        "  folder_dir=\"/content/Data/featureFiles/lexicons\"\n",
        "  for file in os.listdir(folder_dir):\n",
        "      if file.endswith(\"txt\"):\n",
        "        corpus.append(open(os.path.join(folder_dir, file),'r').readlines())\n",
        "  voc={}\n",
        "  count=0\n",
        "  for text in corpus:    \n",
        "      for line in text:\n",
        "        line=line.split(\" --- \")\n",
        "        count+=1\n",
        "        if(len(line)==2):\n",
        "          voc[line[1].strip().lower()]=line[0].lower()\n",
        "  return voc          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-hju6H3UuSI"
      },
      "outputs": [],
      "source": [
        "def Stemmer(word,voc,Translate=Translate,WordStemArabic=WordStemArabic,WordStemEnglish=WordStemEnglish,word2vec=word_embed):\n",
        "  try:\n",
        "    if(word in word2vec.wv.vocab):\n",
        "      return word\n",
        "    elif(re.sub(\"[^\\u0621-\\u064A]+\", '',word) in word2vec.wv.vocab):\n",
        "      return re.sub(\"[^\\u0621-\\u064A]+\", '',word)        \n",
        "    elif(''!= re.sub(\"[^\\u0621-\\u064A]+\", '',word)):\n",
        "      return WordStemArabic(re.sub(\"[^\\u0621-\\u064A]+\", '',word))\n",
        "    elif(''!= re.sub(\"[^a-z]+\", '',word)):\n",
        "       return Translate(re.sub(\"[^a-z]+\", '',word),voc,WordStemEnglish)\n",
        "    return word    \n",
        "  except:\n",
        "     return word"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separating the word from the tag \n",
        "and prepare and stemm the word"
      ],
      "metadata": {
        "id": "574XoyPe5qrd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_znrxysMEOi"
      },
      "outputs": [],
      "source": [
        "def CleanSplitDataLabel(docs,model):\n",
        "  X=[]\n",
        "  Y=[]\n",
        "  for doc in docs:\n",
        "    for line in doc:\n",
        "      line=line.split(' ')\n",
        "      if(len(line)==2):\n",
        "        line[0]=line[0].lower()\n",
        "        line[0]=re.sub(\"[^a-zA-Z\\u0621-\\u064A]\\.\\?\\!+\", '',line[0])\n",
        "        line[0]=Stemmer(line[0],voc)\n",
        "        line[0]=ortho_normalize(line[0])\n",
        "        if(line[0] not in stopwords and line[0]!='' and line[0] in model.wv.vocab):\n",
        "          X.append(line[0])\n",
        "          Y.append(line[1].strip()) \n",
        "  return X,Y    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "make sentence from words"
      ],
      "metadata": {
        "id": "JV4P4nOC6UHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mr9BCoZlN0dZ"
      },
      "outputs": [],
      "source": [
        "def makeSentence(docs,Label):\n",
        "  Sentences=[]\n",
        "  labels=[]\n",
        "  Sentence=[]\n",
        "  label=[]\n",
        "  for i,word in enumerate(docs):\n",
        "    Sentence.append(word)\n",
        "    label.append(Label[i])\n",
        "    if(word=='.' or word=='?' or word=='!'):\n",
        "      Sentences.append(Sentence)\n",
        "      labels.append(label)\n",
        "      Sentence=[]\n",
        "      label=[]\n",
        "  return Sentences,labels   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "حذف التشكيل و الهمزات و تحويل التاء المربوطة الي هاء"
      ],
      "metadata": {
        "id": "86ix9OkO6c48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dV44_BZCre_"
      },
      "outputs": [],
      "source": [
        "def ortho_normalize(text):\n",
        "    text =dediac_ar(text)\n",
        "    text = normalize_alef_maksura_ar(text)\n",
        "    text = normalize_alef_ar(text)\n",
        "    text = normalize_teh_marbuta_ar(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove unnecessary sentences "
      ],
      "metadata": {
        "id": "qpJ0UP7I6rjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv(Sentences,LSentences,window):\n",
        "  sentences=[]\n",
        "  lsentences=[]\n",
        "  for i , sen in enumerate(Sentences):\n",
        "    count=0\n",
        "    while(True):\n",
        "      if(np.unique(LSentences[i][window*count:window*(count+1)]).shape[0]>1):\n",
        "        sentences.append(Sentences[i][window*count:window*(count+1)])\n",
        "        lsentences.append(LSentences[i][window*count:window*(count+1)])\n",
        "      if(len(Sentences[i][window*count:window*(count+1)])!=window):\n",
        "        break  \n",
        "      count+=1  \n",
        "  return sentences,lsentences "
      ],
      "metadata": {
        "id": "r8uuy2KhqjiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for testing"
      ],
      "metadata": {
        "id": "OivoTiKh68ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createInputs(p):\n",
        "  all=[]\n",
        "  for i in p:\n",
        "    pharse=[]\n",
        "    for j in i.split():\n",
        "      pharse.append(j+\" \"+\"O\")\n",
        "    if(pharse[-1]!='.' or pharse[-1]!='?' or pharse[-1]!='!'):\n",
        "      pharse.append('.'+\" \"+\"O\")\n",
        "    all.append(pharse)   \n",
        "  return all    "
      ],
      "metadata": {
        "id": "v6M0ndJwlElN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/full_grams_sg_300_wiki.mdl'\n",
        "word_embed = gensim.models.Word2Vec.load(file_path)"
      ],
      "metadata": {
        "id": "TjdqOTyG13Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window=20\n",
        "allTxt=LoadDataset()\n",
        "voc=Loadlexicons()\n",
        "stopwords=['فى', 'كل', 'لم', 'لن', 'له', 'هو', 'هي', 'قوة', 'كما', 'لها', 'منذ', 'وقد', 'ولا', 'نفسه', 'لقاء', 'مقابل', 'هناك', 'وقال', 'وكان', 'نهاية', 'وقالت', 'وكانت', 'للامم', 'فيه', 'كلم', 'لكن', 'وفي', 'وقف', 'ولم', 'ومن', 'وهو', 'وهي', 'يوم', 'فيها', 'منها', 'مليار', 'لوكالة', 'يكون', 'يمكن', 'مليون', 'حيث', 'اكد', 'الا', 'اما', 'امس', 'السابق', 'التى', 'التي', 'اكثر', 'ايار', 'ايضا', 'ثلاثة', 'الذاتي', 'الاخيرة', 'الثانية', 'الذى', 'الذي', 'امام', 'ايام', 'خلال', 'حوالى', 'الذين', 'الاولى', 'ذلك', 'حول', 'حين', 'الف', 'الى', 'انه', 'اول', 'ضمن', 'انها', 'جميع', 'الماضي', 'الوقت', 'المقبل', 'اليوم', 'ـ', 'ف', 'و6', 'قد', 'لا', 'مع', 'مساء', 'هذا', 'واحد', 'واضاف', 'واضافت', 'قبل', 'قال', 'كان', 'لدى', 'نحو', 'هذه', 'وان', 'واكد', 'كانت', 'واوضح', 'مايو', 'ب', 'ا', 'أ', '،', 'عدد', 'عدة', 'عشرة', 'عدم', 'عام', 'عاما', 'عن', 'عند', 'عندما', 'على', 'عليه', 'عليها', 'زيارة', 'سنة', 'سنوات', 'تم', 'ضد', 'بعد', 'بعض', 'اعادة', 'اعلنت', 'بسبب', 'حتى', 'اذا', 'احد', 'اثر', 'برس', 'باسم', 'غدا', 'شخصا', 'صباح', 'اطار', 'اربعة', 'اخرى', 'بان', 'اجل', 'بشكل', 'حاليا', 'به', 'ثم', 'اف', 'ان', 'بها', 'صفر']"
      ],
      "metadata": {
        "id": "kMFlBPDXcnMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keyed_vectors = word_embed.wv  # structure holding the result of training\n",
        "weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n",
        "index2word = keyed_vectors.index2word  # which row in `weights` corresponds to which word?\n",
        "word2index = {t: i for i, t in enumerate(index2word)} "
      ],
      "metadata": {
        "id": "qgxC3wuMce3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVdn5hDzQ8Ku"
      },
      "outputs": [],
      "source": [
        "X,Y= CleanSplitDataLabel(allTxt,word_embed)\n",
        "tag2idx = {t: i for i, t in enumerate(np.unique(Y))}\n",
        "idx2tag = {i:t for i, t in enumerate(np.unique(Y))}\n",
        "Y=[tag2idx[i] for i in Y]\n",
        "Sentences,LSentences=makeSentence(X,Y)\n",
        "Sentences,LSentences=conv(Sentences,LSentences,window)\n",
        "Sentences=[[word2index.get(word) for word in line] for line in Sentences]\n",
        "pad_vectors = pad_sequences(Sentences, maxlen=window, padding='post',dtype='int', value=word2index.get(\".\"))\n",
        "pad_labels = pad_sequences(LSentences, maxlen=window, padding='post',dtype='int', value=tag2idx['O'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVUqgre9dv1m"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(pad_vectors, pad_labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_word = Input(shape=(window))\n",
        "embedding_layer = Embedding(input_dim=len(word2index),\n",
        "                            output_dim=300,\n",
        "                            weights=[weights],\n",
        "                            input_length=window,\n",
        "                            trainable=False)(input_word)\n",
        "model = LSTM(units=100, return_sequences=True, recurrent_dropout=0.5)(embedding_layer)\n",
        "model = LSTM(units=150, return_sequences=True, recurrent_dropout=0.5)(model)\n",
        "out = TimeDistributed(Dense(22, activation=\"softmax\"))(model)\n",
        "model = Model(input_word, out)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIaQCFFsc6uL",
        "outputId": "39448ee8-089b-45f3-84a5-7b7b78416ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 20, 300)           198632700 \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 20, 100)           160400    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 20, 150)           150600    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 20, 22)           3322      \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 198,947,022\n",
            "Trainable params: 314,322\n",
            "Non-trainable params: 198,632,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePvqjEs4Ywfp"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOaITw9-Y2oR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18950f83-848b-4b65-b345-ac2339530ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "120/120 [==============================] - 24s 146ms/step - loss: 0.9433 - accuracy: 0.8321 - val_loss: 0.6394 - val_accuracy: 0.8443\n",
            "Epoch 2/40\n",
            "120/120 [==============================] - 18s 148ms/step - loss: 0.5676 - accuracy: 0.8548 - val_loss: 0.4905 - val_accuracy: 0.8756\n",
            "Epoch 3/40\n",
            "120/120 [==============================] - 21s 176ms/step - loss: 0.4633 - accuracy: 0.8797 - val_loss: 0.4133 - val_accuracy: 0.8921\n",
            "Epoch 4/40\n",
            "120/120 [==============================] - 21s 175ms/step - loss: 0.3997 - accuracy: 0.8925 - val_loss: 0.3717 - val_accuracy: 0.9009\n",
            "Epoch 5/40\n",
            "120/120 [==============================] - 19s 157ms/step - loss: 0.3632 - accuracy: 0.9002 - val_loss: 0.3488 - val_accuracy: 0.9057\n",
            "Epoch 6/40\n",
            "120/120 [==============================] - 18s 150ms/step - loss: 0.3341 - accuracy: 0.9076 - val_loss: 0.3418 - val_accuracy: 0.9099\n",
            "Epoch 7/40\n",
            "120/120 [==============================] - 19s 162ms/step - loss: 0.3180 - accuracy: 0.9111 - val_loss: 0.3232 - val_accuracy: 0.9122\n",
            "Epoch 8/40\n",
            "120/120 [==============================] - 21s 172ms/step - loss: 0.2952 - accuracy: 0.9169 - val_loss: 0.3103 - val_accuracy: 0.9147\n",
            "Epoch 9/40\n",
            "120/120 [==============================] - 19s 157ms/step - loss: 0.2741 - accuracy: 0.9209 - val_loss: 0.3031 - val_accuracy: 0.9173\n",
            "Epoch 10/40\n",
            "120/120 [==============================] - 21s 171ms/step - loss: 0.2583 - accuracy: 0.9244 - val_loss: 0.2922 - val_accuracy: 0.9203\n",
            "Epoch 11/40\n",
            "120/120 [==============================] - 23s 193ms/step - loss: 0.2379 - accuracy: 0.9294 - val_loss: 0.2909 - val_accuracy: 0.9188\n",
            "Epoch 12/40\n",
            "120/120 [==============================] - 22s 181ms/step - loss: 0.2267 - accuracy: 0.9323 - val_loss: 0.2930 - val_accuracy: 0.9156\n",
            "Epoch 13/40\n",
            "120/120 [==============================] - 17s 146ms/step - loss: 0.2102 - accuracy: 0.9359 - val_loss: 0.2845 - val_accuracy: 0.9214\n",
            "Epoch 14/40\n",
            "120/120 [==============================] - 18s 150ms/step - loss: 0.1942 - accuracy: 0.9403 - val_loss: 0.2877 - val_accuracy: 0.9194\n",
            "Epoch 15/40\n",
            "120/120 [==============================] - 17s 140ms/step - loss: 0.1848 - accuracy: 0.9433 - val_loss: 0.2933 - val_accuracy: 0.9218\n",
            "Epoch 16/40\n",
            "120/120 [==============================] - 17s 143ms/step - loss: 0.1690 - accuracy: 0.9464 - val_loss: 0.2887 - val_accuracy: 0.9200\n",
            "Epoch 17/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.1574 - accuracy: 0.9512 - val_loss: 0.2864 - val_accuracy: 0.9222\n",
            "Epoch 18/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.1454 - accuracy: 0.9543 - val_loss: 0.2901 - val_accuracy: 0.9223\n",
            "Epoch 19/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.1342 - accuracy: 0.9584 - val_loss: 0.2884 - val_accuracy: 0.9216\n",
            "Epoch 20/40\n",
            "120/120 [==============================] - 17s 143ms/step - loss: 0.1217 - accuracy: 0.9625 - val_loss: 0.2943 - val_accuracy: 0.9225\n",
            "Epoch 21/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.1104 - accuracy: 0.9651 - val_loss: 0.3007 - val_accuracy: 0.9223\n",
            "Epoch 22/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.0990 - accuracy: 0.9700 - val_loss: 0.3036 - val_accuracy: 0.9232\n",
            "Epoch 23/40\n",
            "120/120 [==============================] - 17s 140ms/step - loss: 0.0894 - accuracy: 0.9717 - val_loss: 0.3158 - val_accuracy: 0.9229\n",
            "Epoch 24/40\n",
            "120/120 [==============================] - 17s 140ms/step - loss: 0.0837 - accuracy: 0.9732 - val_loss: 0.3165 - val_accuracy: 0.9236\n",
            "Epoch 25/40\n",
            "120/120 [==============================] - 17s 140ms/step - loss: 0.0776 - accuracy: 0.9752 - val_loss: 0.3218 - val_accuracy: 0.9161\n",
            "Epoch 26/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.0657 - accuracy: 0.9793 - val_loss: 0.3415 - val_accuracy: 0.9222\n",
            "Epoch 27/40\n",
            "120/120 [==============================] - 17s 139ms/step - loss: 0.0631 - accuracy: 0.9793 - val_loss: 0.3333 - val_accuracy: 0.9190\n",
            "Epoch 28/40\n",
            "120/120 [==============================] - 17s 138ms/step - loss: 0.0596 - accuracy: 0.9812 - val_loss: 0.3817 - val_accuracy: 0.9219\n",
            "Epoch 29/40\n",
            "120/120 [==============================] - 17s 138ms/step - loss: 0.0517 - accuracy: 0.9834 - val_loss: 0.3629 - val_accuracy: 0.9220\n",
            "Epoch 30/40\n",
            "120/120 [==============================] - 16s 136ms/step - loss: 0.0445 - accuracy: 0.9869 - val_loss: 0.3706 - val_accuracy: 0.9224\n",
            "Epoch 31/40\n",
            "120/120 [==============================] - 17s 138ms/step - loss: 0.0383 - accuracy: 0.9887 - val_loss: 0.3630 - val_accuracy: 0.9208\n",
            "Epoch 32/40\n",
            "120/120 [==============================] - 18s 150ms/step - loss: 0.0417 - accuracy: 0.9875 - val_loss: 0.3667 - val_accuracy: 0.9182\n",
            "Epoch 33/40\n",
            "120/120 [==============================] - 18s 146ms/step - loss: 0.0382 - accuracy: 0.9883 - val_loss: 0.3796 - val_accuracy: 0.9212\n",
            "Epoch 34/40\n",
            "120/120 [==============================] - 17s 145ms/step - loss: 0.0377 - accuracy: 0.9876 - val_loss: 0.3788 - val_accuracy: 0.9195\n",
            "Epoch 35/40\n",
            "120/120 [==============================] - 23s 195ms/step - loss: 0.0276 - accuracy: 0.9920 - val_loss: 0.4015 - val_accuracy: 0.9215\n",
            "Epoch 36/40\n",
            "120/120 [==============================] - 17s 142ms/step - loss: 0.0272 - accuracy: 0.9920 - val_loss: 0.4224 - val_accuracy: 0.9218\n",
            "Epoch 37/40\n",
            "120/120 [==============================] - 16s 138ms/step - loss: 0.0242 - accuracy: 0.9928 - val_loss: 0.3998 - val_accuracy: 0.9193\n",
            "Epoch 38/40\n",
            "120/120 [==============================] - 18s 150ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 0.4274 - val_accuracy: 0.9223\n",
            "Epoch 39/40\n",
            "120/120 [==============================] - 25s 208ms/step - loss: 0.0216 - accuracy: 0.9933 - val_loss: 0.4132 - val_accuracy: 0.9203\n",
            "Epoch 40/40\n",
            "120/120 [==============================] - 19s 154ms/step - loss: 0.0208 - accuracy: 0.9934 - val_loss: 0.4490 - val_accuracy: 0.9220\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x=X_train,y=y_train,batch_size=16,validation_data=(X_test,y_test), epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"mymodelv2.h5\")"
      ],
      "metadata": {
        "id": "w9H-ziiZ17kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1=\"محمد سافر الي لبنان\"\n",
        "p2=\"منتخب مصر سيواجه منتخب السنغال\"\n",
        "p3=\"كرة القدم رياضة جماعية و ليست فردية\"\n",
        "p4=\"تعد مستشفى مصطفى محمود تابعة للجمعية التي اسسها الدكتور مصطفى محمود رحمه الله\"\n",
        "p5=\"صلاح الدين الأيوبي قائد مسلم أسس الدولة الأيوبية التي وحدت مصر والشام والحجاز وتهامة واليمن في ظل الراية العباسية\"\n",
        "p6=\"يحكم بروتوكول العلاج الكيميائي على استخدام أكثر من دواء مختلفين بآلية العمل والهدف من ذلك هو تقليل مقاومة الخلايا السرطانية\"\n",
        "p7=\"القصف الذرّي على هيروشيما وناجازاكي هو هجوم نووي شنته الولايات المتحدة ضد الإمبراطورية اليابانية في نهاية الحرب العالمية الثانية\"\n",
        "p8=\"اغلب المبرمجين في الماضي اختاروا تجاهل مسائل الشبكة و استخدام واجهات برمجة الصوت المحلية و الخاصة بكل نظام\""
      ],
      "metadata": {
        "id": "a84pNb4bEIoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pharses=[p1,p2,p3,p4,p5,p6,p7,p8]"
      ],
      "metadata": {
        "id": "wqADqCpi1tun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all=createInputs(pharses)\n",
        "X1,Y1= CleanSplitDataLabel(all,word_embed)\n",
        "Sentences,LSentences=makeSentence(X1,Y1)\n",
        "Sentences1=[[word2index.get(word) for word in line] for line in Sentences]\n",
        "pad_vectors = pad_sequences(Sentences1, maxlen=window, padding='post',dtype='int', value=word2index.get(\".\"))"
      ],
      "metadata": {
        "id": "oiNTTUIsFtoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=model.predict(pad_vectors)"
      ],
      "metadata": {
        "id": "j4wsfSa9GAWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output=np.argmax(output,axis=-1)"
      ],
      "metadata": {
        "id": "l_wSG2ixrmy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(pharses)):\n",
        "  for j in range(len(pharses[i].split())):\n",
        "    print(pharses[i].split()[j],\" \",idx2tag[output[i][j]])\n",
        "  print(\"=================================\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQcIpvrgovqT",
        "outputId": "7ebc3476-4a82-4036-9791-b263ac20ce27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "محمد   B-PER\n",
            "سافر   O\n",
            "الي   O\n",
            "لبنان   B-LOC\n",
            "=================================\n",
            "منتخب   B-ORG\n",
            "مصر   I-ORG\n",
            "سيواجه   O\n",
            "منتخب   B-ORG\n",
            "السنغال   I-ORG\n",
            "=================================\n",
            "كرة   B-MIS1\n",
            "القدم   I-MIS1\n",
            "رياضة   O\n",
            "جماعية   O\n",
            "و   O\n",
            "ليست   O\n",
            "فردية   O\n",
            "=================================\n",
            "تعد   O\n",
            "مستشفى   O\n",
            "مصطفى   I-LOC\n",
            "محمود   I-LOC\n",
            "تابعة   O\n",
            "للجمعية   O\n",
            "التي   O\n",
            "اسسها   O\n",
            "الدكتور   B-PER\n",
            "مصطفى   I-PER\n",
            "محمود   I-PER\n",
            "رحمه   I-PER\n",
            "الله   O\n",
            "=================================\n",
            "صلاح   B-PER\n",
            "الدين   I-PER\n",
            "الأيوبي   I-PER\n",
            "قائد   O\n",
            "مسلم   O\n",
            "أسس   O\n",
            "الدولة   O\n",
            "الأيوبية   B-PER\n",
            "التي   O\n",
            "وحدت   B-LOC\n",
            "مصر   B-LOC\n",
            "والشام   B-LOC\n",
            "والحجاز   B-LOC\n",
            "وتهامة   O\n",
            "واليمن   O\n",
            "في   B-PER\n",
            "ظل   O\n",
            "الراية   O\n",
            "العباسية   O\n",
            "=================================\n",
            "يحكم   O\n",
            "بروتوكول   B-MIS1\n",
            "العلاج   O\n",
            "الكيميائي   O\n",
            "على   O\n",
            "استخدام   O\n",
            "أكثر   O\n",
            "من   O\n",
            "دواء   O\n",
            "مختلفين   O\n",
            "بآلية   O\n",
            "العمل   O\n",
            "والهدف   O\n",
            "من   O\n",
            "ذلك   O\n",
            "هو   O\n",
            "تقليل   O\n",
            "مقاومة   O\n",
            "الخلايا   O\n",
            "السرطانية   O\n",
            "=================================\n",
            "القصف   O\n",
            "الذرّي   O\n",
            "على   O\n",
            "هيروشيما   B-LOC\n",
            "وناجازاكي   B-LOC\n",
            "هو   O\n",
            "هجوم   O\n",
            "نووي   O\n",
            "شنته   B-LOC\n",
            "الولايات   I-LOC\n",
            "المتحدة   B-LOC\n",
            "ضد   O\n",
            "الإمبراطورية   O\n",
            "اليابانية   O\n",
            "في   O\n",
            "نهاية   O\n",
            "الحرب   O\n",
            "العالمية   O\n",
            "الثانية   O\n",
            "=================================\n",
            "اغلب   O\n",
            "المبرمجين   O\n",
            "في   O\n",
            "الماضي   O\n",
            "اختاروا   O\n",
            "تجاهل   O\n",
            "مسائل   O\n",
            "الشبكة   O\n",
            "و   O\n",
            "استخدام   B-MIS1\n",
            "واجهات   I-MIS1\n",
            "برمجة   O\n",
            "الصوت   O\n",
            "المحلية   O\n",
            "و   O\n",
            "الخاصة   O\n",
            "بكل   O\n",
            "نظام   O\n",
            "=================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}